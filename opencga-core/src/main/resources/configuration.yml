---
logLevel: "INFO"
logDir: ${OPENCGA.INSTALLATION.DIR}/logs

openRegister: false

databasePrefix: ${OPENCGA.DB.PREFIX}
workspace: ${OPENCGA.USER.WORKSPACE}
jobDir: ${OPENCGA.USER.WORKSPACE}/jobs

panel:
  host: "http://resources.opencb.org/opencb/opencga/disease-panels"

## Configuration for Catalog databases
catalog:
  database:   # MongoDB database credentials
    hosts:
    - ${OPENCGA.CATALOG.DB.HOSTS}
    user: ${OPENCGA.CATALOG.DB.USER}
    password: ${OPENCGA.CATALOG.DB.PASSWORD}
    options:
      authenticationDatabase: ${OPENCGA.CATALOG.DB.AUTHENTICATION_DATABASE}
      connectionsPerHost: ${OPENCGA.CATALOG.DB.CONNECTIONS_PER_HOST}
  ## Solr Search engine configuration, by default is the same than storage
  searchEngine:
    # List of hosts pointing either to the Solr nodes directly using a complete URL or to the zookeper nodes with HOST:PORT
    #    Example for Solr connection:       http://opencga-solr-01.zone:8983/solr
    #    Example for Zookeeper connection:  opencga-zookeeper-01:2181               <-- Recommended for replicated installations
    hosts:
    - ${OPENCGA.CATALOG.SEARCH.HOST}
    user: ""
    password: ""
    options:
      mode: "cloud"
      timeout: ${OPENCGA.CATALOG.SEARCH.TIMEOUT}
      insertBatchSize: ${OPENCGA.CATALOG.SEARCH.BATCH}

## We support multiple Authentication providers, if none is provided then we use an internal authentication implementation
authentication:
  # Session expiration time in seconds
  expiration: 3600
  authenticationOrigins:
# LDAP configuration example
#  - id: ldap            # Any id
#    type: LDAP
#    host: ldap://localhost:9000
#    options:
#      usersSearch: dc=ge,dc=co,dc=uk # Base search to look for the users
#      groupsSearch: ou=general,ou=groups,dc=ge,dc=co,dc=uk # Base search to look for the groups

# Custom LDAPS configuration example
#  - id: ldaps            # Any id
#    type: LDAP
#    host: ldaps://localhost:636
#    options:
#      usersSearch: dc=ge,dc=co,dc=uk                       # Base search to look for users
#      groupsSearch: ou=general,ou=groups,dc=ge,dc=co,dc=uk # Base search to look for groups
#      fullNameKey: displayname                             # Key to get the user's full name when importing users
#      dnKey: gecos                                         # Get the user's unique identifier...
#      dnFormat: "uid=%s,ou=people,dc=ge,dc=co,dc=uk"       # ...and pass a distinguished name to the auth request in the right format

# Azure AD configuration example
#  - id: aad                                               # Any id
#    type: AzureAD
#    host:
#    options:
#      tenantId: xxxx              # Mandatory. Tenant id
#      authClientId: xxxx          # Mandatory. Client id of the client with permissions to authenticate users.
#      syncClientId: xxxx          # Mandatory. Client id of the client with permissions to inspect active directory.
#      syncSecretKey: xxxx         # Mandatory: Secret key of the client with permissions to inspect active directory.
#      filters: tokenField1=aa,bb,cc;tokenField2=aa,bb,cc  # Optional. Filters to be applied. OpenCGA will check if tokenField1 = aa or bb
#                # or cc and tokenField2 = aa or bb or cc. If any of the filters don't succeed, even if the user is properly authenticated
#                # in AAD, the user will not be able to generate a token and login in OpenCGA.

server:
  rest:
    port: ${OPENCGA.SERVER.REST.PORT}
    logFile: null
    defaultLimit: 2000
    maxLimit: 5000
  grpc:
    port: ${OPENCGA.SERVER.GRPC.PORT}
    logFile: null

audit:
  manager: ""             # Java manager of the audit implementation to be used to audit. If empty, catalog database will be used.
  maxDocuments: 20000000  # Maximum number of documents that will be created in the audit collection.
  maxSize: 100            # Maximum size that the audit collection will have in Gigabytes (GB).

monitor:
  daysToRemove: 30
  executionDaemonInterval: 4000 # number of milliseconds between checks
  fileDaemonInterval: 8000      # number of milliseconds between checks
  port: ${OPENCGA.MONITOR.PORT}

healthCheck:
  interval : 30 # seconds to get actual healthCheck than cache

#execution:
#  mode: ${OPENCGA.EXECUTION.MODE}
#  maxConcurrentIndexJobs : 1 # only applies to local executor
#  defaultQueue: ""
#  availableQueues: ""
#  toolsPerQueue: {}
#  options:


analysis:
  scratchDir: "${OPENCGA.ANALYSIS.SCRATCH.DIR}"    # Scratch folder for the analysis.
  execution:
    # Accepted values are "local", "SGE", "azure-batch", "k8s"
    # see org.opencb.opencga.master.monitor.executors.ExecutorFactory
    id: "${OPENCGA.EXECUTION.MODE}"
    defaultQueue: ""            # Default queue to be used to submit jobs
    availableQueues:            # Other queues for specific applications
    toolsPerQueue:
#       docker:
#          - "circos"
#          - "deeptools"
#          - "bwa"
#          - "fastqc"
#          - "gatk"
#          - "picard"
#          - "plink"
#          - "rvtests"
#          - "samtools"
#          - "relatedness"
#          - "family-qc"
#          - "sample-qc"
#          - "individual-qc"
#          - "mutational-signature"
#       fast:
#          - "alignmentIndex"
#       slow:
#          - "coverage"
#          - "alignmentStats"
    maxConcurrentJobs:
      variant-index: 20
      variant-annotation-index: 5
      variant-secondary-index: 2
    options:
     ## Local executor configuration
      local.maxConcurrentJobs: 1    # Max number of concurrent jobs to be executed locally in the master
     ## Azure Batch Service configuration example
     # azure.batchAccount : "batchAccount"
     # azure.batchKey : "batchKey"
     # azure.batchUri : "https://batchservice.uksouth.batch.azure.com"
     # azure.batchPoolId : "poolId"
     # azure.dockerImageName : "openCGADockerImageName"
     # azure.dockerArgs : "dockerRunOptions"

     ## Kubernetes executor configuration example
     # k8s.masterUrl: "https://192.168.99.100:8443/"
     # FOR Cluster Autoscaler and dedicated jobs agent pool:
      k8s.imageName: "opencb/opencga-base:2.0.0-hdp3.1-dev"
      k8s.namespace: "default"
      k8s.requests:
        cpu: 3
        memory: "12G"
      k8s.limits:
        cpu: 4
        memory: "13G"
      k8s.nodeSelector:
        agentpool: jobs
        beta.kubernetes.io/os: linux
        kubernetes.io/role: agent
     # FOR ACI:
     # k8s.requests:
     #   cpu: 2
     #   memory: "12G"
     # k8s.limits:
     #   cpu: 2
     #   memory: "12G"
     # k8s.nodeSelector:
     #   kubernetes.io/role: agent
     #   beta.kubernetes.io/os: linux
     #   type: virtual-kubelet
     # k8s.tolerations:
     # - key: virtual-kubelet.io/provider
     #   operator: Exists
     # - key: azure.com/aci
     #   effect: NoSchedule
      k8s.volumeMounts:
        - name: conf
          mountPath: /opt/opencga/conf
        - name: sessions
          mountPath: /opt/opencga/sessions
        - name: variants
          mountPath: /opt/opencga/variants
        - name: analysisconf
          mountPath: /opt/opencga/analysis
      k8s.volumes:
        - name: conf
          azureFile:
            secretName: azure-files-secret
            shareName: conf
            readOnly: true
        - name: sessions
          azureFile:
            secretName: azure-files-secret
            shareName: sessions
            readOnly: false
        - name: variants
          azureFile:
            secretName: azure-files-secret
            shareName: variants
            readOnly: false
        - name: analysisconf
          azureFile:
            secretName: azure-files-secret
            shareName: analysisconf
            readOnly: false

  # List of analysis frameworks
  frameworks:
    - id: "local"
      available: true
      options: {}

    - id: "spark"
      available: false
      queue: "hadoop_queue" # Special executor queue to be used for jobs using this framework.
      options:     # Spark properties from https://spark.apache.org/docs/latest/configuration.html#viewing-spark-properties
        spark.executor.memory: "1g"

    - id: "mapreduce"
      available: false
      queue: "hadoop_queue" # Special executor queue to be used for jobs using this framework. This is NOT a yarn queue.
      options:   # MapReduce configuration from https://hadoop.apache.org/docs/r2.7.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml
        mapreduce.job.queuename: default

email:
  host: ${OPENCGA.MAIL.HOST}
  port: ${OPENCGA.MAIL.PORT}
  user: ${OPENCGA.MAIL.USER}
  password: ${OPENCGA.MAIL.PASSWORD}
  from: ""
  ssl: false

#hooks:
#  user@project:study:              # Full Qualified Name of the study.
#    file:                          # Entity where the hook will be checked
#     - field: "name"               # Field of the entity to be checked
#       value: "~(.*)SV.vcf.gz$"    # Value that needs to be satisfied to perform the hook action
#       stage: "CREATE"             # Stage when the hook will be checked
#       action: "ADD"               # Action to be performed
#       where: "tags"               # Field over which the action will be performed
#       what: "SV"                  # Value to be updated
